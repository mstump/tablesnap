#!/usr/bin/env python
import StringIO
import boto
import grp
import json
import logging
import os
import os.path
import pwd
import pyinotify
import signal
import socket
import sys
import time

from functools import *
from optparse import OptionParser
from traceback import format_exc
from threading import Thread
from Queue import Queue


# Default number of writer threads
DEFAULT_THREADS = 4

# Default retries
DEFAULT_RETRIES = 3

# S3 limit for single file upload
S3_LIMIT = 5 * 2**30

# Max file size to upload without doing multipart in bytes
MAX_FILE_SIZE = 4120 * 2**20

# Default chunk size for multipart uploads in MB
DEFAULT_CHUNK_SIZE = 256 * 2**20

TIMEOUT = 5 * 60

def get_log(debug=False):
    log = logging.getLogger('tablesnap')
    stderr = logging.StreamHandler()
    stderr.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))
    log.addHandler(stderr)
    if debug:
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)
    return log

def md5sum(key, filename):
    try:
        f = open(filename, mode='rb')
        return key.compute_md5(f)
    finally:
        f.close()

class UploadHandler(pyinotify.ProcessEvent):
    def my_init(self, log=None, threads=None, key=None, secret=None, bucket_name=None,
                prefix=None, name=None, max_size=MAX_FILE_SIZE, chunk_size=DEFAULT_CHUNK_SIZE):
        self.key = key
        self.secret = secret
        self.bucket_name = bucket_name
        self.prefix = prefix
        self.name = name or socket.getfqdn()
        self.retries = DEFAULT_RETRIES
        self.log = log
        self.max_size = max_size
        self.chunk_size = chunk_size

        self.fileq = Queue()
        for i in range(int(threads)):
            t = Thread(target=self.worker)
            t.daemon = True
            t.start()

    def build_keyname(self, pathname):
        return '%s%s:%s' % (self.prefix or '', self.name, pathname)

    def add_file(self, filename):
        if filename.find('-tmp') == -1:
            self.log.debug("adding %s to the upload queue" % filename)
            self.fileq.put(filename)

    def get_bucket(self):
        # Reconnect to S3
        s3 = boto.connect_s3(self.key, self.secret)
        return s3.get_bucket(self.bucket_name)

    def retry_with_new_bucket(self, f):
        count = self.retries

        def wrapper(*args, **kwds):
            while(True):
                try:
                    args[0] = self.get_bucket()
                    return f(*args, **kwds)
                except:
                    count
                    if not count > 0:
                        self.log.error("Exceeded retry attempts")
                        self.log.error(traceback.format_exc())
                        return None
        return wrapper

    def worker(self):
        bucket = self.get_bucket()

        while True:
            f = self.fileq.get()
            keyname = self.build_keyname(f)
            try:
                self.retry_with_new_bucket(upload_sstable)(bucket, self.log, keyname, f, chunk_size=self.chunk_size, max_size=self.max_size)
            except:
                self.log.critical("Failed uploading %s. Aborting.\n%s" % (f, format_exc()))
                os.kill(os.getpid(), signal.SIGKILL) # Brute force kill self

            self.fileq.task_done()

    def process_IN_MOVED_TO(self, event):
        self.add_file(event.pathname)

def key_exists(bucket, log, keyname, filename, stat):
    """Check if this keyname (ie, file) has already been uploaded to
    the S3 bucket. This will verify that not only does the keyname
    exist, but that the MD5 sum is the same -- this protects against
    partial or corrupt uploads."""
    key = bucket.get_key(keyname)
    if key == None:
        log.debug('Key %s does not exist' % (keyname,))
        return False

    if key.size != stat.st_size:
        log.warning('ATTENTION: your source (%s) and target (%s) '
                    'sizes differ, you should take a look. As immutable files '
                    'never change, one must assume the local file got corrupted '
                    'and the right version is the one in S3. Will skip this file '
                    'to avoid future complications' % (filename, keyname, ))
        return True

    # Compute MD5 sum of file
    md5 = md5sum(key, filename)
    log.debug('Computed md5: %s' % (md5,))
    log.debug('ETag comparison: %s == %s? : %s' % (md5[0], key.etag.strip('"'), (md5[0] == key.etag.strip('"'))))

    if (md5[0] != key.etag.strip('"')):
        log.warning('ATTENTION: your source (%s) and target (%s) '
                    'MD5 hashes differ, you should take a look. As immutable '
                    'files never change, one must assume the local file got '
                    'corrupted and the right version is the one in S3. Will '
                    'skip this file to avoid future complications' %
                    (filename, keyname, ))
        return True

    log.info("Keyname %s already exists, skipping upload" % (keyname))
    return False

def upload_sstable(bucket, log, keyname, filename, with_index=True, chunk_size=DEFAULT_CHUNK_SIZE, max_size=MAX_FILE_SIZE):
    """Include the file system metadata so that we have the
    option of using it to restore the file modes correctly."""
    try:
        stat = os.stat(filename)
    except OSError:
        return # File removed?

    try:
        if key_exists(bucket, log, keyname, filename, stat):
            return
        dirname = os.path.dirname(filename)
        meta = json.dumps(get_meta(stat))

        if with_index:
            upload_index(bucket, log, keyname, dirname)

        log.debug('File size check: %s > %s ? : %s' % (stat.st_size, max_size, (stat.st_size > max_size),))
        if stat.st_size > max_size:
            upload_multipart(bucket, log, keyname, meta, chunk_size, filename)
        else:
            upload_monolithic(bucket, log, keyname, meta, filename)

    except:
        log.error('Error uploading %s\n%s' % (keyname, format_exc()))
        raise

def upload_index(bucket, log, keyname, dirname):
    json_str = json.dumps({dirname: os.listdir(dirname)})
    key = bucket.new_key('%s-listdir.json' % keyname)
    key.set_contents_from_string(json_str,
                                 headers={'Content-Type': 'application/json'},
                                 replace=True)

def get_meta(stat):
    meta = {'uid': stat.st_uid,
            'gid': stat.st_gid,
            'mode': stat.st_mode}
    try:
        u = pwd.getpwuid(stat.st_uid)
        meta['user'] = u.pw_name
    except:
        pass

    try:
        g = grp.getgrgid(stat.st_gid)
        meta['group'] = g.gr_name
    except:
        pass

def progress(log, filename, sent, total):
    if sent == total:
        log.info('Finished uploading %s' % filename)

def get_free_memory_in_kb():
    f = open('/proc/meminfo', 'r')
    memlines = f.readlines()
    f.close()
    lines = []
    for line in memlines:
        ml = line.rstrip(' kB\n').split(':')
        lines.append((ml[0], int(ml[1].strip())))
    d = dict(lines)
    return d['Cached'] + d['MemFree'] + d['Buffers']

def split_sstable(log, chunk_size, filename):
    free = get_free_memory_in_kb() * 1024
    log.debug('Free memory check: %d < %d ? : %s' % (free, chunk_size, (free < chunk_size)))
    if free < chunk_size:
        log.warn('Your system is low on memory, reading in smaller chunks')
        chunk_size = free / 20
    else:
        chunk_size = chunk_size
    log.debug('Reading %s in %d byte sized chunks' % (filename, chunk_size))
    f = open(filename, 'rb')
    while True:
        chunk = f.read(chunk_size)
        if chunk:
            yield StringIO.StringIO(chunk)
        else:
            break
    if f and not f.closed:
        f.close()

def upload_monolithic(bucket, log, keyname, meta, filename):
    log.debug('Performing monolithic upload')
    key = bucket.new_key(keyname)
    key.set_metadata('stat', meta)
    key.set_contents_from_filename(filename, replace=True, cb=partial(progress, log, filename), num_cb=1)

def upload_multipart(bucket, log, keyname, meta, chunk_size, filename):
    log.info('Performing multipart upload for %s' % (filename))
    mp = bucket.initiate_multipart_upload(keyname, metadata={'stat': meta})
    part = 1
    chunk = None
    try:
        for chunk in split_sstable(log, chunk_size, filename):
            log.debug('Uploading part #%d (size: %d)' % (part, chunk.len,))
            mp.upload_part_from_file(chunk, part)
            chunk.close()
            part += 1

        log.debug('Uploaded %d parts, completing upload' % (part - 1,))
        mp.complete_upload()
        progress(log, filename, 100, 100)
    except Exception as e:
        log.debug(e)
        log.info('Error uploading part %d' % (part,))
        mp.cancel_upload()
        if chunk:
            chunk.close()
        raise

def backup_file(handler, filename, filedir):
    if filename.find('-tmp') != -1:
        return
    fullpath = '%s/%s' % (filedir, filename)
    if os.path.isdir(fullpath):
        return

    handler.add_file(fullpath)

def backup_files(handler, paths, recurse, log):
    for path in paths:
        log.info('Backing up %s' % path)
        if recurse:
            for root, dirs, files in os.walk(path):
                for filename in files:
                    backup_file(handler, filename, root)
        else:
            for filename in os.listdir(path):
                backup_file(handler, filename, path)
    return 0


def main():
    parser = OptionParser(usage='%prog [options] <bucket> <path> [...]')
    parser.add_option('-v', '--verbose', dest='verbose', default=None)
    parser.add_option('-k', '--aws-key', dest='aws_key', default=None)
    parser.add_option('-s', '--aws-secret', dest='aws_secret', default=None)
    parser.add_option('-r', '--recursive', action='store_true', dest='recursive', default=False,
                      help='Recursively watch the given path(s)s for new SSTables')
    parser.add_option('-a', '--auto-add', action='store_true', dest='auto_add', default=False,
                      help='Automatically start watching new subdirectories within path(s)')
    parser.add_option('-p', '--prefix', dest='prefix', default=None,
                      help='Set a string prefix for uploaded files in S3')
    parser.add_option('-t', '--threads', dest='threads', default=DEFAULT_THREADS,
                      help='Number of writer threads')
    parser.add_option('-n', '--name', dest='name', default=None,
                      help='Use this name instead of the FQDN to identify the SSTables from this host')
    parser.add_option('--max-upload-size', dest='max_upload_size',
                      default=MAX_FILE_SIZE / 2**20,
                      help='Max size for files to be uploaded before doing multipart (default %dM)' % (MAX_FILE_SIZE / 2**20))
    parser.add_option('--multipart-chunk-size', dest='multipart_chunk_size',
                      default=DEFAULT_CHUNK_SIZE / 2**20,
                      help='Chunk size for multipart uploads (default: %dM or 10%% of free memory if default is not available)' % (DEFAULT_CHUNK_SIZE / 2**20,))
    options, args = parser.parse_args()

    if len(args) < 2:
        parser.print_help()
        return -1

    bucket = args[0]
    paths = args[1:]

    # Check S3 credentials only. We reconnect per-thread to avoid any
    # potential thread-safety problems.
    s3 = boto.connect_s3(options.aws_key, options.aws_secret)
    bucket = s3.get_bucket(bucket)
    log = get_log(options.verbose)

    handler = UploadHandler(log=log, threads=options.threads, key=options.aws_key,
                            secret=options.aws_secret, bucket_name=bucket,
                            prefix=options.prefix, name=options.name,
                            max_size=int(options.max_upload_size) * 2**20,
                            chunk_size=int(options.multipart_chunk_size) * 2**20)

    wm = pyinotify.WatchManager()
    notifier = pyinotify.Notifier(wm, handler)
    for path in paths:
        ret = wm.add_watch(path, pyinotify.IN_MOVED_TO, rec=options.recursive, auto_add=options.auto_add)
        if ret[path] == -1:
            get_log().critical('add_watch failed for %s, bailing out!' % (path))
            return 1

    first_iteration = True
    while True:
        try:
            ref_time = time.time()

            if not first_iteration:
                notifier.process_events()
                if notifier.check_events():
                    time.sleep(TIMEOUT - (time.time() - ref_time))
                    notifier.read_events()
                first_iteration = False

            # to catch failed backups
            backup_files(handler, paths, options.recursive, log)

        except KeyboardInterrupt:
            log.debug('Pyinotify stops monitoring.')
            break

    notifier.stop()


if __name__ == '__main__':
    sys.exit(main())
